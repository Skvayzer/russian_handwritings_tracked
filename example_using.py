# -*- coding: utf-8 -*-
"""Yacenko Samsung MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L287ro1tkK6LuJdGepi-K_bxVmwh_DGm
"""

import torch
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split

def fixSeeds():
    # To make random weights in NN consistent from launch to launch on this device
    # To actually distinguish an improvement
    random.seed(0)
    np.random.seed(0)
    torch.manual_seed(0)
    torch.cuda.manual_seed(0)
    torch.backends.cudnn.deterministic = True


fixSeeds()


# !unzip /content/drive/MyDrive/ML/yacenko-dataset/tablet_renamed_duplicated.zip

def sort(lst):
    lst = [str(i) for i in lst]
    lst.sort()
    lst = [int(i) if i.isdigit() else i for i in lst]
    return lst


upper_alphabet = ['Ё',
                  'А',
                  'Б',
                  'В',
                  'Г',
                  'Д',
                  'Е',
                  'Ж',
                  'З',
                  'И',
                  'Й',
                  'К',
                  'Л',
                  'М',
                  'Н',
                  'О',
                  'П',
                  'Р',
                  'С',
                  'Т',
                  'У',
                  'Ф',
                  'Х',
                  'Ц',
                  'Ч',
                  'Ш',
                  'Щ',
                  'Ъ',
                  'Ы',
                  'Ь',
                  'Э',
                  'Ю',
                  'Я']



subfolders = [f.path for f in os.scandir(".") if f.is_dir()]
print(subfolders)

X = None
y = None
SYMBOLS = []
for i, w in enumerate(subfolders):
    filenames = next(os.walk(w + "/mnist-like"), (None, None, []))[2]  # [] if no file
    filenames = sort(filenames)
    if w == './.git': continue
    if i == 0:
        SYMBOLS = [x.split('.')[0] for x in filenames]
        X = torch.empty(0, len(SYMBOLS), 28, 28)
        y = torch.empty(0, len(SYMBOLS))

    temp_train = torch.empty(0, 28, 28)
    temp_val = torch.empty(0)
    for j, file in enumerate(filenames):
        data = np.genfromtxt(w + "/mnist-like/" + file, delimiter=',')
        data = torch.unsqueeze(torch.tensor(data), dim=0)
        label = file.split(".")[0].lower()
        if label == '0': label = 'о'
        # if label in upper_alphabet: continue
        temp_train = torch.cat((temp_train, torch.tensor(data)), dim=0)
        item = torch.tensor([SYMBOLS.index(label)])
        temp_val = torch.cat((temp_val, item), dim=0)
    temp_train = torch.unsqueeze(temp_train, dim=0)
    temp_val = torch.unsqueeze(temp_val, dim=0)

    X = torch.cat((X, temp_train), dim=0)
    y = torch.cat((y, temp_val), dim=0)



X = X.reshape([-1, 28, 28])
y = y.reshape(-1)
print(X.shape)
test_size = 0.1
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

X = X.to(device)
y = y.to(device).float()
X = X.reshape(X.shape[0], 1, 28, 28)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
y_test = y_test.long()


class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()

        # 28x28x1 => 26x26x32
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3)
        self.d1 = nn.Linear(26 * 26 * 64, 512)
        self.d2 = nn.Linear(512, len(SYMBOLS))

    def forward(self, x):
        # 32x1x28x28 => 32x32x26x26
        x = self.conv1(x)
        x = F.relu(x)

        # flatten => 32 x (32*26*26)
        x = x.flatten(start_dim=1)

        # 32 x (32*26*26) => 32x128
        x = self.d1(x)
        x = F.relu(x)

        # logits => 32x10
        logits = self.d2(x)
        out = F.softmax(logits, dim=1)
        return out


mnistNet = ConvNet()

mnistNet = mnistNet.to(device)
mnistNet.parameters()

loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(mnistNet.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)

batch_size = 100
test_accuracy_history = []
test_loss_history = []
train_loss_history = []

for epoch in range(200):
    loss_value = 0
    for i in range(0, len(X_train), batch_size):
        optimizer.zero_grad()
        X_batch = X_train[i: i + batch_size].float().to(device)
        y_batch = y_train[i: i + batch_size].long().to(device)

        X_batch = X_batch.reshape(X_batch.shape[0], 1, 28, 28)

        prediction = mnistNet.forward(X_batch)
        loss_value = loss(prediction, y_batch)
        loss_value.backward()
        optimizer.step()
    scheduler.step()

    train_loss_history.append(loss_value.item())
    test_prediction = mnistNet.forward(X_test.float())

    accuracy = (test_prediction.argmax(dim=1) == y_test.int()).float().mean().item()
    print(accuracy)
    test_loss_history.append(loss(test_prediction, y_test).item())
    test_accuracy_history.append(accuracy)

max(test_accuracy_history)

plt.plot(train_loss_history)
plt.plot(test_loss_history)
plt.show()




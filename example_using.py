# -*- coding: utf-8 -*-
"""Yacenko Samsung MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L287ro1tkK6LuJdGepi-K_bxVmwh_DGm
"""

import torch
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F


def fixSeeds():
    # To make random weights in NN consistent from launch to launch on this device
    # To actually distinguish an improvement
    random.seed(0)
    np.random.seed(0)
    torch.manual_seed(0)
    torch.cuda.manual_seed(0)
    torch.backends.cudnn.deterministic = True


fixSeeds()


# !unzip /content/drive/MyDrive/ML/yacenko-dataset/tablet_renamed_duplicated.zip

def sort(lst):
    lst = [str(i) for i in lst]
    lst.sort()
    lst = [int(i) if i.isdigit() else i for i in lst]
    return lst


upper_alphabet = ['Ё',
                  'А',
                  'Б',
                  'В',
                  'Г',
                  'Д',
                  'Е',
                  'Ж',
                  'З',
                  'И',
                  'Й',
                  'К',
                  'Л',
                  'М',
                  'Н',
                  'О',
                  'П',
                  'Р',
                  'С',
                  'Т',
                  'У',
                  'Ф',
                  'Х',
                  'Ц',
                  'Ч',
                  'Ш',
                  'Щ',
                  'Ъ',
                  'Ы',
                  'Ь',
                  'Э',
                  'Ю',
                  'Я']


# not using upper case letters for learning
subfolders = [f.path for f in os.scandir(".") if f.is_dir()]
print(subfolders)
dataset = torch.empty(0, 43, 28, 28)
SYMBOLS = []
for i, w in enumerate(subfolders):
    symbols = []
    filenames = next(os.walk(w + "/mnist-like"), (None, None, []))[2]  # [] if no file
    filenames = sort(filenames)

    # if i==0: SYMBOLS = [x.split('.')[0] for x in filenames]

    temp_files = torch.empty(0, 28, 28)
    for j, file in enumerate(filenames):
        data = np.genfromtxt(w + "/mnist-like/" + file, delimiter=',')
        data = torch.unsqueeze(torch.tensor(data), dim=0)
        label = file.split(".")[0]
        if label in upper_alphabet: continue
        if i == 0: SYMBOLS.append(label)
        # label = int(label) if label.isdigit() else 10 + ord(label) - ord('А')
        temp_files = torch.cat((temp_files, torch.tensor(data)), dim=0)
    temp_files = torch.unsqueeze(temp_files, dim=0)
    dataset = torch.cat((dataset, temp_files), dim=0)




dataset = dataset.reshape([-1, 28, 28])

test_size = 0.1
train_set, val_set = dataset[: len(dataset) - int(test_size * len(dataset)), :, :], dataset[len(dataset) - int(
    test_size * len(dataset)):, :, :]
len(train_set), len(val_set)



device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")



# class PerpcetronNet(nn.Module):
#     def __init__(self, input_neurons, hidden_neurons):
#         super(PerpcetronNet, self).__init__()
#         self.input_neurons = input_neurons
#         self.hidden_neurons = hidden_neurons
#
#         self.fc1 = nn.Linear(input_neurons, hidden_neurons)
#         self.al1 = nn.Sigmoid()
#         self.fc2 = nn.Linear(hidden_neurons, 76)
#
#     def forward(self, x):
#         x = self.fc1(x)
#         x = self.al1(x)
#         x = self.fc2(x)
#         return x




class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()

        # 28x28x1 => 26x26x32
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3)
        self.d1 = nn.Linear(26 * 26 * 64, 512)
        self.d2 = nn.Linear(512, len(SYMBOLS))

    def forward(self, x):
        # 32x1x28x28 => 32x32x26x26
        x = self.conv1(x)
        x = F.relu(x)

        # flatten => 32 x (32*26*26)
        x = x.flatten(start_dim=1)

        # 32 x (32*26*26) => 32x128
        x = self.d1(x)
        x = F.relu(x)

        # logits => 32x10
        logits = self.d2(x)
        out = F.softmax(logits, dim=1)
        return out


mnistNet = ConvNet()

mnistNet = mnistNet.to(device)
mnistNet.parameters()

loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(mnistNet.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)

batch_size = 100
train_set = train_set.to(device)
val_set = val_set.to(device).float()
val_set = val_set.reshape(val_set.shape[0], 1, 28, 28)
test_accuracy_history = []
test_loss_history = []
train_loss_history = []

for epoch in range(200):
    # permutation = np.random.permutation(len(train_set))
    loss_value = 0
    for i in range(0, len(train_set), batch_size):
        optimizer.zero_grad()
        X_batch = train_set[i: i + batch_size].float().to(device)
        y_batch = torch.tensor([y % len(SYMBOLS) for y in
                                range(i, i + batch_size if i + batch_size <= len(train_set) else len(train_set))]).to(
            device)

        X_batch = X_batch.reshape(X_batch.shape[0], 1, 28, 28)
        # print(X_batch.shape)
        # plt.imshow(X_batch[67][0].reshape((28, 28)).cpu())
        # plt.show()
        # print(SYMBOLS[y_batch[67]])

        prediction = mnistNet.forward(X_batch)
        loss_value = loss(prediction, y_batch)
        loss_value.backward()
        optimizer.step()
    scheduler.step()

    train_loss_history.append(loss_value.item())
    # print(val_set.shape)
    test_prediction = mnistNet.forward(val_set.float())
    y_test = torch.tensor([y % len(SYMBOLS) for y in range(len(train_set), len(train_set) + len(val_set))]).to(device)
    accuracy = (test_prediction.argmax(dim=1) == y_test).float().mean().item()
    print(accuracy)
    test_loss_history.append(loss(test_prediction, y_test).item())
    test_accuracy_history.append(accuracy)

max(test_accuracy_history)

plt.plot(train_loss_history)
plt.plot(test_loss_history)
plt.show()



